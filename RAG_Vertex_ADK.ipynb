{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafab21-74da-4d91-82fa-f387bf9dc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import AsyncGenerator, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google.adk.agents import BaseAgent, LlmAgent, LoopAgent, SequentialAgent\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from google.adk.events import Event, EventActions\n",
    "from google.adk.tools import VertexAiSearchTool\n",
    "from google.genai.types import Content, Part\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the datastore ID from environment\n",
    "datastore_id = os.getenv(\n",
    "    \"VERTEX_SEARCH_DATASTORE_ID\",\n",
    "    \"projects/YOUR_PROJECT_ID/locations/YOUR_LOCATION/collections/default_collection/dataStores/YOUR_DATASTORE_ID\",\n",
    ")\n",
    "\n",
    "# Create the Vertex AI Search tool instance\n",
    "vertex_search_tool = VertexAiSearchTool(\n",
    "    data_store_id=datastore_id,\n",
    ")\n",
    "\n",
    "\n",
    "APP_NAME = \"deep_research_agent\"\n",
    "USER_ID = \"research_user_01\"\n",
    "SESSION_ID_BASE = \"research_session\"\n",
    "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "STATE_QUERIES = \"queries\"\n",
    "STATE_SEARCH_RESULTS = \"search_results\"\n",
    "STATE_REFLECTION = \"reflection\"\n",
    "STATE_FINAL_ANSWER = \"final_answer\"\n",
    "STATE_RESEARCH_LOOP_COUNT = \"research_loop_count\"\n",
    "STATE_QUERY_RATIONALE = \"query_rationale\"\n",
    "\n",
    "# Configuration\n",
    "MAX_RESEARCH_LOOPS = 2\n",
    "NUMBER_OF_INITIAL_QUERIES = 3\n",
    "\n",
    "\n",
    "def get_current_date():\n",
    "    \"\"\"Get current date in a readable format\"\"\"\n",
    "    return datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    rationale: str = Field(\n",
    "        description=\"Brief explanation of why these queries are relevant\"\n",
    "    )\n",
    "    queries: List[str] = Field(description=\"A list of search queries\")\n",
    "\n",
    "\n",
    "class ReflectionResult(BaseModel):\n",
    "    is_sufficient: bool = Field(\n",
    "        description=\"Is the information sufficient to answer the original question?\"\n",
    "    )\n",
    "    follow_up_queries: Optional[List[str]] = Field(\n",
    "        default=None, description=\"New search queries if more research needed\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SetupAgent(BaseAgent):\n",
    "    \"\"\"Persists the user's initial question to state at workflow start\"\"\"\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        user_question = \"\"\n",
    "        if ctx.user_content and ctx.user_content.parts:\n",
    "            user_question = ctx.user_content.parts[0].text or \"\"\n",
    "\n",
    "        yield Event(\n",
    "            author=self.name,\n",
    "            content=Content(\n",
    "                parts=[Part(text=\"Initializing deep research workflow...\")]\n",
    "            ),\n",
    "            actions=EventActions(state_delta={\"user_question\": user_question}),\n",
    "        )\n",
    "\n",
    "\n",
    "class QueryGeneratorAgent(BaseAgent):\n",
    "    \"\"\"Generates sophisticated search queries and saves to state\"\"\"\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        user_question = ctx.session.state.get(\"user_question\", \"\")\n",
    "\n",
    "        query_agent = LlmAgent(\n",
    "            name=\"QueryLLM\",\n",
    "            model=GEMINI_MODEL,\n",
    "            instruction=f\"\"\"Your goal is to generate sophisticated and diverse web search queries for: {user_question}\n",
    "\n",
    "**Current Date:** {get_current_date()}\n",
    "\n",
    "**Instructions:**\n",
    "- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\n",
    "- Each query should focus on one specific aspect of the original question.\n",
    "- Don't produce more than {NUMBER_OF_INITIAL_QUERIES} queries.\n",
    "- Queries should be diverse, if the topic is broad, generate more than 1 query.\n",
    "- Don't generate multiple similar queries, 1 is enough.\n",
    "- Query should ensure that the most current information is gathered.\n",
    "\n",
    "**Format:**\n",
    "Output your response as a JSON object with these exact keys:\n",
    "- \"rationale\": Brief explanation of why these queries are relevant\n",
    "- \"queries\": A list of search queries (as strings)\n",
    "\n",
    "**Example:**\n",
    "```json\n",
    "{{\n",
    "    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple's stock performance and iPhone sales metrics. These queries target the precise financial information needed.\",\n",
    "    \"queries\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\"]\n",
    "}}\n",
    "```\n",
    "\n",
    "Generate sophisticated search queries for the user's question.\"\"\",\n",
    "            output_schema=SearchQueries,\n",
    "        )\n",
    "\n",
    "        async for event in query_agent.run_async(ctx):\n",
    "            if event.author == \"QueryLLM\" and event.content:\n",
    "                try:\n",
    "                    content_text = event.content.parts[0].text\n",
    "                    queries_data = json.loads(content_text)\n",
    "\n",
    "                    yield Event(\n",
    "                        author=self.name,\n",
    "                        content=Content(\n",
    "                            parts=[\n",
    "                                Part(\n",
    "                                    text=f\"Generated {len(queries_data['queries'])} search queries: {', '.join(queries_data['queries'])}\"\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                        actions=EventActions(state_delta={STATE_QUERIES: queries_data}),\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    yield Event(\n",
    "                        author=self.name,\n",
    "                        content=Content(\n",
    "                            parts=[Part(text=f\"Error parsing query generation: {e}\")]\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "\n",
    "class SearchAgent(BaseAgent):\n",
    "    \"\"\"Executes searches and saves results to state\"\"\"\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        queries_data = ctx.session.state.get(STATE_QUERIES, {})\n",
    "        existing_results = ctx.session.state.get(STATE_SEARCH_RESULTS, \"\")\n",
    "\n",
    "        queries = (\n",
    "            queries_data.get(\"queries\", []) if isinstance(queries_data, dict) else []\n",
    "        )\n",
    "\n",
    "        search_agent = LlmAgent(\n",
    "            name=\"SearchLLM\",\n",
    "            model=GEMINI_MODEL,\n",
    "            instruction=f\"\"\"Conduct targeted searches to gather the most recent, credible information and synthesize it into a verifiable text artifact.\n",
    "\n",
    "**Current Date:** {get_current_date()}\n",
    "\n",
    "**Current Queries:** {\", \".join(queries)}\n",
    "\n",
    "**Existing Research (if any):** {existing_results}\n",
    "\n",
    "**Instructions:**\n",
    "- Query should ensure that the most current information is gathered.\n",
    "- Conduct comprehensive searches to gather comprehensive information.\n",
    "- Consolidate key findings while meticulously tracking the source(s) for each specific piece of information.\n",
    "- The output should be a well-written summary or report based on your search findings.\n",
    "- Only include the information found in the search results, don't make up any information.\n",
    "- If building upon existing research, integrate new findings with previous results while avoiding duplication.\n",
    "\n",
    "**Output Format:**\n",
    "Organize results by:\n",
    "- Key findings and insights\n",
    "- Supporting evidence and data\n",
    "- Source information and credibility\n",
    "- Relevance to the original question\n",
    "\n",
    "Provide a comprehensive compilation of all search results.\"\"\",\n",
    "            tools=[vertex_search_tool],\n",
    "        )\n",
    "\n",
    "        async for event in search_agent.run_async(ctx):\n",
    "            if event.author == \"SearchLLM\" and event.content:\n",
    "                search_results = event.content.parts[0].text\n",
    "                yield Event(\n",
    "                    author=self.name,\n",
    "                    content=Content(\n",
    "                        parts=[\n",
    "                            Part(text=\"Research completed - consolidating findings...\")\n",
    "                        ]\n",
    "                    ),\n",
    "                    actions=EventActions(\n",
    "                        state_delta={STATE_SEARCH_RESULTS: search_results}\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "\n",
    "class ReflectionAgent(BaseAgent):\n",
    "    \"\"\"Evaluates research sufficiency and either escalates or generates follow-up queries\"\"\"\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        user_question = ctx.session.state.get(\"user_question\", \"\")\n",
    "        search_results = ctx.session.state.get(STATE_SEARCH_RESULTS, \"\")\n",
    "\n",
    "        reflection_agent = LlmAgent(\n",
    "            name=\"ReflectionLLM\",\n",
    "            model=GEMINI_MODEL,\n",
    "            instruction=f\"\"\"You are an expert research assistant analyzing research summaries for: {user_question}\n",
    "\n",
    "**Current Date:** {get_current_date()}\n",
    "\n",
    "**Research Results:**\n",
    "{search_results}\n",
    "\n",
    "**Task:** Evaluate if the current research provides sufficient context to answer the user's question comprehensively.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. **Completeness**: Do we have enough information to fully address the question?\n",
    "2. **Quality**: Are the sources credible and authoritative?\n",
    "3. **Coverage**: Are all key aspects of the question addressed?\n",
    "4. **Depth**: Do we have sufficient detail for a comprehensive answer?\n",
    "5. **Currency**: Is the information current and up-to-date?\n",
    "\n",
    "**Output Format:**\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "    \"is_sufficient\": true/false,\n",
    "    \"follow_up_queries\": [\"query1\", \"query2\"] // only if is_sufficient is false\n",
    "}}\n",
    "\n",
    "Be decisive. If sufficient, set is_sufficient to true. If not, provide 2-3 targeted follow-up queries.\"\"\",\n",
    "            output_schema=ReflectionResult,\n",
    "        )\n",
    "\n",
    "        async for event in reflection_agent.run_async(ctx):\n",
    "            if event.author == \"ReflectionLLM\" and event.content:\n",
    "                try:\n",
    "                    content_text = event.content.parts[0].text\n",
    "                    reflection_data = json.loads(content_text)\n",
    "\n",
    "                    if reflection_data.get(\"is_sufficient\", True):\n",
    "                        yield Event(\n",
    "                            author=self.name,\n",
    "                            content=Content(\n",
    "                                parts=[\n",
    "                                    Part(\n",
    "                                        text=\"Information is sufficient. Proceeding to final summary.\"\n",
    "                                    )\n",
    "                                ]\n",
    "                            ),\n",
    "                            actions=EventActions(escalate=True),\n",
    "                        )\n",
    "                    else:\n",
    "                        follow_up = reflection_data.get(\"follow_up_queries\", [])\n",
    "                        yield Event(\n",
    "                            author=self.name,\n",
    "                            content=Content(\n",
    "                                parts=[\n",
    "                                    Part(\n",
    "                                        text=f\"Information not sufficient. New queries: {follow_up}\"\n",
    "                                    )\n",
    "                                ]\n",
    "                            ),\n",
    "                            actions=EventActions(\n",
    "                                state_delta={STATE_QUERIES: {\"queries\": follow_up}}\n",
    "                            ),\n",
    "                        )\n",
    "                except Exception:\n",
    "                    # Default to escalation if parsing fails\n",
    "                    yield Event(\n",
    "                        author=self.name,\n",
    "                        content=Content(\n",
    "                            parts=[\n",
    "                                Part(\n",
    "                                    text=\"Reflection analysis complete. Proceeding to summary.\"\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                        actions=EventActions(escalate=True),\n",
    "                    )\n",
    "\n",
    "\n",
    "research_loop = LoopAgent(\n",
    "    name=\"ResearchLoop\",\n",
    "    sub_agents=[\n",
    "        SearchAgent(name=\"SearchAgent\"),\n",
    "        ReflectionAgent(name=\"ReflectionAgent\"),\n",
    "    ],\n",
    "    max_iterations=MAX_RESEARCH_LOOPS,  # Maximum research iterations\n",
    ")\n",
    "\n",
    "\n",
    "class AnswerGeneratorAgent(BaseAgent):\n",
    "    \"\"\"Generates and displays comprehensive final answer directly to user\"\"\"\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        user_question = ctx.session.state.get(\"user_question\", \"\")\n",
    "        search_results = ctx.session.state.get(STATE_SEARCH_RESULTS, \"\")\n",
    "\n",
    "        answer_agent = LlmAgent(\n",
    "            name=\"AnswerLLM\",\n",
    "            model=\"gemini-2.5-pro\",  # Use more powerful model for final answer\n",
    "            instruction=f\"\"\"Generate a high-quality answer to the user's question: {user_question}\n",
    "\n",
    "**Current Date:** {get_current_date()}\n",
    "\n",
    "**Research Summaries:**\n",
    "{search_results}\n",
    "\n",
    "**Instructions:**\n",
    "- You are the final step of a multi-step research process\n",
    "- You have access to all information gathered from previous research steps\n",
    "- Generate a high-quality, comprehensive answer based on the research summaries\n",
    "- Structure your response professionally with clear sections and flow\n",
    "- Include sources and citations where applicable\n",
    "- Be thorough but concise\n",
    "\n",
    "**Answer Structure:**\n",
    "1. **Direct Response**: Lead with a clear, concise answer to the question\n",
    "2. **Comprehensive Analysis**: Provide detailed information with supporting evidence\n",
    "3. **Key Insights**: Highlight the most important findings\n",
    "4. **Supporting Evidence**: Reference specific data and sources from research\n",
    "5. **Context & Implications**: Provide broader context and practical implications\n",
    "6. **Conclusion**: Summarize key takeaways\n",
    "\n",
    "**Format Requirements:**\n",
    "- Use clear markdown formatting with headers and sections\n",
    "- Include bullet points and lists for clarity\n",
    "- Bold key findings and important information\n",
    "- Professional, authoritative tone\n",
    "- Comprehensive yet accessible language\n",
    "\n",
    "**Quality Standards:**\n",
    "- Ensure accuracy by only including information from research summaries\n",
    "- Provide balanced perspective when multiple viewpoints exist\n",
    "- Address all aspects of the original question\n",
    "- Maintain logical flow and organization\n",
    "\n",
    "Generate a comprehensive, well-structured answer that fully addresses the user's question.\"\"\",\n",
    "        )\n",
    "\n",
    "        async for event in answer_agent.run_async(ctx):\n",
    "            if event.author == \"AnswerLLM\" and event.content:\n",
    "                final_answer = event.content.parts[0].text\n",
    "                yield Event(\n",
    "                    author=\"\",\n",
    "                    content=Content(parts=[Part(text=final_answer)]),\n",
    "                    turn_complete=True,\n",
    "                )\n",
    "\n",
    "\n",
    "root_agent = SequentialAgent(\n",
    "    name=\"EnhancedDeepResearchAgent\",\n",
    "    sub_agents=[\n",
    "        SetupAgent(name=\"SetupAgent\"),\n",
    "        QueryGeneratorAgent(name=\"QueryGeneratorAgent\"),\n",
    "        research_loop,\n",
    "        AnswerGeneratorAgent(name=\"AnswerGeneratorAgent\"),\n",
    "    ],\n",
    "    description=\"Enhanced deep research agent with sophisticated prompts, knowledge gap analysis, and structured outputs\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
